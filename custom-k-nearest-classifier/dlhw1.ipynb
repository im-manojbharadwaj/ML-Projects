{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243eb434-f0c1-4bf2-bac6-be1e0f6b90bc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    " <h1>IMPLEMENTING K-NEAREST-NEIGHBOUR (KNN) ALGORITHM FOR IMAGE CLASSIFICATION ON CIFAR-10 DATASET</h1>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img width = 500 src=\"./KNN.jpg\">\n",
    "</div>\n",
    "\n",
    "<h4>The KNN algorithm is a simple and effective method for classifying data based on the majority class of its nearest neighbors.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65fe8f-0e48-4ff4-9cfd-5703366e78b1",
   "metadata": {},
   "source": [
    "## DRIVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e021f5-6ea1-43fc-8e07-87ba6d031537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import datasets\n",
    "from scipy.spatial.distance import cdist\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921d244-37da-4b7b-9822-1240c9d1aea4",
   "metadata": {},
   "source": [
    "## USER DEFINED FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed13aa4-ff3f-437e-8046-578e7c1592d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Define transformations to be applied to the images\n",
    "def imageTransform():\n",
    "    tform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "    ])\n",
    "    return tform\n",
    "\n",
    "# Function to Download CIFAR-10 dataset\n",
    "def downloadDataset():\n",
    "    transform = imageTransform()\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Function to create data loaders for training and testing\n",
    "def traintestLoaders(train_dataset, test_dataset):\n",
    "    train_dataset, test_dataset = train_dataset, test_dataset\n",
    "    \n",
    "    # Create data loaders for training and testing\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Extract features and labels for training set\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        # Flatten the images into vectors\n",
    "        images = images.view(images.size(0), -1)\n",
    "        train_features.append(images)\n",
    "        train_labels.append(labels)\n",
    "\n",
    "    train_features = torch.cat(train_features, dim=0)\n",
    "    train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "    # Extract features and labels for test set\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        # Flatten the images into vectors\n",
    "        images = images.view(images.size(0), -1)\n",
    "        test_features.append(images)\n",
    "        test_labels.append(labels)\n",
    "    \n",
    "    test_features = torch.cat(test_features, dim=0)\n",
    "    test_labels = torch.cat(test_labels, dim=0)\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "# Function to convert tensors to numpy arrays\n",
    "def convertTensorsToNumpy():\n",
    "    return train_features.numpy(), test_features.numpy(), train_labels.numpy(), test_labels.numpy()\n",
    "\n",
    "# Function to reduce the sample size for custom KNN Classifier\n",
    "def reduceSampleSize(sample_size, features, labels):\n",
    "    # Shuffle your data\n",
    "    f, l = shuffle(features, labels)\n",
    "    \n",
    "    # Sample without replacement\n",
    "    indices = np.random.randint(low=0, high=len(f), size=sample_size)\n",
    "\n",
    "    # Use indices to select subset\n",
    "    reduced_features = f[indices]\n",
    "    reduced_labels = l[indices]\n",
    "\n",
    "    return reduced_features, reduced_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e659c-d261-4c4f-984f-981ae32fe1df",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6b4d2b-f1f1-4356-8f77-d0331e9e9b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Transformation and Loading is completed. Ready for training the model\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "train_dataset, test_dataset = downloadDataset()\n",
    "\n",
    "# Splitting the dataset\n",
    "train_features, test_features, train_labels, test_labels = traintestLoaders(train_dataset, test_dataset)\n",
    "\n",
    "# Converting to NumPy type\n",
    "train_features, test_features, train_labels, test_labels = convertTensorsToNumpy()\n",
    "\n",
    "#confirmation\n",
    "print('Transformation and Loading is completed. Ready for training the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fadd2ef-af02-4b4a-a24d-0a60fd933980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on test set: 33.03%\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = knn_classifier.predict(test_features)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f'Accuracy of KNN classifier on test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900ccd2-8069-4c93-a0dc-bf4593dc8c60",
   "metadata": {},
   "source": [
    "# BUILDING CUSTOM KNN-CLASSIFIER TO WORK ON CIFAR-10 DATASET\n",
    "\n",
    "The KNN algorithm is a simple and effective method for classifying data based on the majority class of its nearest neighbors.\n",
    "\n",
    "The custom KNN classifier is implemented in the Python programming language. The class, named KNN_Classifier, is designed to be flexible and easy to use. The key components of the implementation include:\n",
    "\n",
    "<b>Initialization:</b><br>\n",
    "The class is initialized with a parameter `k`, which represents the number of neighbors to consider during classification. The default value is set to `5`, a common choice in practice.\n",
    "\n",
    "<b>Model Fitting:</b><br>\n",
    "The `fitModel` method is responsible for fitting the model with training data. It takes training features `X` and corresponding labels `y` as input and stores them internally.\n",
    "\n",
    "<b>Prediction:</b><br>\n",
    "The `predict` method is used to make predictions for new data points. Given a set of test points `X`, it iterates through each test point, computes the distances to all training points, identifies the indices of the k-nearest neighbors, and predicts the label based on the majority class.\n",
    "\n",
    "<b>Distance Computation:</b><br>\n",
    "The `compute_distances` method calculates the distances between a test point and all training points using the Euclidean distance metric. The `cdist` function from the `scipy.spatial.distance` module is utilized for efficiency.\n",
    "\n",
    "<b>Performance Analysis:</b><br>\n",
    "The performance of the custom KNN classifier can be assessed based on metrics such as accuracy, precision, recall, and F1 score. These metrics can be calculated by comparing the predicted labels to the true labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6fec70d-a956-4bb9-92fa-2957902fe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom KNN Classifier class\n",
    "class KNN_Classifier:\n",
    "\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fitModel(self, X, y):\n",
    "        self.X_train = X \n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for i, test_point in enumerate(X):\n",
    "            distances = self.compute_distances(test_point)\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_neighbor_labels = [self.y_train[i] for i in k_indices]  \n",
    "            most_common = Counter(k_neighbor_labels).most_common(1)\n",
    "            y_pred[i] = most_common[0][0]\n",
    "        return y_pred\n",
    "\n",
    "    def compute_distances(self, test_point):\n",
    "        return cdist(np.array([test_point]), self.X_train).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000dd711-e92f-48b5-92c2-b3cd3ef6204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the sample size for training and testing to suit the custom KNN Classifier\n",
    "reduced_train_features, reduced_train_labels = reduceSampleSize(20000, train_features, train_labels)\n",
    "reduced_test_features, reduced_test_labels = reduceSampleSize(250, test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c5b076-6d42-4fea-b0d2-0421bd7990d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 83 / 250 correct => accuracy: 33.200000\n"
     ]
    }
   ],
   "source": [
    "# Using custom KNN classifier\n",
    "custom_knn_classifier = KNN_Classifier()\n",
    "custom_knn_classifier.fitModel(reduced_train_features, reduced_train_labels)\n",
    "\n",
    "y_pred = custom_knn_classifier.predict(reduced_test_features)\n",
    "\n",
    "num_correct = np.sum(y_pred == reduced_test_labels)\n",
    "accuracy = (num_correct) / (reduced_test_features.shape[0])\n",
    "\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, reduced_test_features.shape[0], accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902eaa23-37c3-4f4d-80b9-14c596d8c048",
   "metadata": {},
   "source": [
    "We can see that the custom KNN classifier that I have written gives the accuracy of 33.2%. But this can also depend on various factors because we have taken only a portion of the images by reducing the sample size to 20000 for traning and testing it on 250 samples only.\n",
    "\n",
    "But neverthless, let's try to increase the accuracy by tuning the hyper-parameters and adding some weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53118dc2-f540-4773-b9f1-fccb75801a2e",
   "metadata": {},
   "source": [
    "# OPTIMISING THE CUSTOM KNN CLASSIFIER\n",
    "\n",
    "<h4>\n",
    "    When we have to deal with optimizing the custom KNN class, we need to experiment with different aspects of your algorithm\n",
    "</h4>\n",
    "\n",
    "\n",
    "<b>Distance Weights:</b><br>\n",
    "Introduce distance weights when computing predictions. You can use the inverse of the distances as weights, giving more importance to closer neighbors.\n",
    "\n",
    "<b>Use a Different Distance Metric:</b><br>\n",
    "Experiment with different distance metrics. You might want to try Manhattan distance `(p=1)` or Minkowski distance with different `p` values.\n",
    "\n",
    "<b>Kernel Density Estimation:</b><br>\n",
    "Implement kernel density estimation for a more robust estimation of the underlying density function.\n",
    "\n",
    "<b>Use a Larger Dataset:</b><br>\n",
    "If possible, use a larger dataset for training. More data can often lead to better generalization.\n",
    "\n",
    "<b>Implement Parallelization:</b><br>\n",
    "Depending on the size of your dataset, you might benefit from parallelizing the distance calculations. This can be achieved using parallel computing libraries such as `joblib` or `concurrent.futures`.\n",
    "\n",
    "<b>Optimize Code for Efficiency:</b><br>\n",
    "Ensure that your code is optimized for efficiency. Use vectorized operations wherever possible to speed up computations.\n",
    "\n",
    "<b>Optimize `k` Value:</b><br>\n",
    "Experiment with different values of `k` to find the one that gives the best performance. You can use techniques like grid search or random search for hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abaa025e-aad3-4072-9467-31f7510c980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier_Optimized:\n",
    "\n",
    "    def __init__(self, k=4, weights='uniform', distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.weights = weights\n",
    "        self.distance_metric = distance_metric\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fitModel(self, X, y):\n",
    "        # Scale the features\n",
    "        self.X_train = self.scaler.fit_transform(X)\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Scale the test features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        y_pred = np.zeros(len(X))\n",
    "        for i, test_point in enumerate(X_scaled):\n",
    "            distances = self.compute_distances(test_point)\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_neighbor_labels = [self.y_train[i] for i in k_indices]  \n",
    "\n",
    "            if self.weights == 'uniform':\n",
    "                most_common = Counter(k_neighbor_labels).most_common(1)\n",
    "            else:\n",
    "                weights = 1 / (distances[k_indices] + 1e-5)  # Add a small epsilon to avoid division by zero\n",
    "                most_common = Counter(dict(zip(k_neighbor_labels, weights))).most_common(1)\n",
    "\n",
    "            y_pred[i] = most_common[0][0]\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def compute_distances(self, test_point):\n",
    "        return cdist(np.array([test_point]), self.X_train, metric=self.distance_metric).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1432b2e4-cefa-4ecd-bea4-07caafc07f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 86 / 250 correct => accuracy: 34.400000\n"
     ]
    }
   ],
   "source": [
    "# Using optimized custom KNN classifier\n",
    "custom_knn_classifier_o = KNN_Classifier_Optimized()\n",
    "custom_knn_classifier_o.fitModel(reduced_train_features, reduced_train_labels)\n",
    "\n",
    "y_pred_o = custom_knn_classifier_o.predict(reduced_test_features)\n",
    "\n",
    "num_correct = np.sum(y_pred_o == reduced_test_labels)\n",
    "accuracy = (num_correct) / (reduced_test_features.shape[0])\n",
    "\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, reduced_test_features.shape[0], accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8209a-23e1-42c0-bccb-039d161ff91e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06138795-5259-434b-aff9-a2a1f1dca838",
   "metadata": {},
   "source": [
    "# RESULT\n",
    "\n",
    "            Classifier Name                           Accuracy\n",
    "    Scikit-Learn In-Built KNN Classifier               33.03%\n",
    "    Custom KNN Classifier                              33.20%\n",
    "    Custom KNN Classifier Optimized                    34.40\n",
    "\n",
    "By updating the custom KNN Classifier with above mentioned optimization methods, we were able to achieve the accuracy 34.40% beating the in-built Scikit-Learn KNN classifier. It may not be huge difference but on a larger scale it gives a better impact on handling the dataset and predicting using test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
